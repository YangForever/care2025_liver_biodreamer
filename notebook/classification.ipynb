{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170322ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import os\n",
    "from augmentation import *\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "mod1 = 'DWI_800.nii.gz'\n",
    "mod2 = 'GED1.nii.gz'\n",
    "mod3 = 'GED2.nii.gz'\n",
    "mod4 = 'GED3.nii.gz'\n",
    "mod5 = 'GED4.nii.gz'\n",
    "mod6 = 'T1.nii.gz'\n",
    "mod7 = 'T2.nii.gz'\n",
    "mod8 = 'mask_GED4.nii.gz'\n",
    "\n",
    "mods = [mod1, mod2, mod3, mod4, mod5, mod6, mod7, mod8]\n",
    "\n",
    "vendor_to_number = {'A': 0, 'B1':1, 'B2':2, 'C':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0296baf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the path to the main folder\n",
    "search_directory = Path('C://Users//SCoulY//Downloads//care2025_liver_biodreamer//data')\n",
    "\n",
    "# Recursively find all files ending with '.nii.gz'\n",
    "# The rglob('*.nii.gz') method returns a generator\n",
    "mod_list = []\n",
    "for mod in mods:\n",
    "    file_list = list(search_directory.rglob(f'*{os.sep}{mod}'))\n",
    "    mod_list.append(file_list)\n",
    "    print(f\"Mod {mod} found {len(file_list)} files.\")\n",
    "\n",
    "mask_list = []\n",
    "search_dir = Path('C://Users//SCoulY//Downloads//train_val_all_data_all_modalities//train_all_modalities')\n",
    "for mask in search_dir.rglob('*.nii.gz'):\n",
    "    mask_list.append(mask)\n",
    "print(f\"Found {len(mask_list)} segmentation masks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545325c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair the samples with segmentation masks\n",
    "mask_str_list = [str(mask.parent).split(os.sep)[-1] + os.sep + mask.name for mask in mask_list]\n",
    "\n",
    "mod_roi_list = []\n",
    "for sample in tqdm(mod_list[5]): #only T1\n",
    "    mod_name = sample.name\n",
    "    sample_name = str(sample.parent).split(os.sep)[-1]  # Get the last part of the path as sample name\n",
    "    # print(f\"Processing sample: {sample_name} with mod: {mod_name}\")\n",
    "    mask_name = sample_name + os.sep + 'T1_pred.nii.gz'  # Assuming mask corresponds to DWI_800\n",
    "    \n",
    "    if mask_name in mask_str_list:\n",
    "        idx = mask_str_list.index(mask_name)\n",
    "        mask = nib.load(mask_list[idx]).get_fdata().astype(np.float32)\n",
    "        img = nib.load(str(sample).replace(mod_name, 'T1.nii.gz')).get_fdata().astype(np.float32)\n",
    "        roi = img[mask>0]\n",
    "        if roi.size == 0:\n",
    "            print(f\"No ROI found for sample: {sample_name} with mod: {mod_name}\")\n",
    "            continue\n",
    "        mod_roi_list.append((img, mask, roi, sample_name))\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32466a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "#calculate morphology features for each ROI\n",
    "from skimage import measure, morphology\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.stats import skew, kurtosis\n",
    "from skimage.feature.texture import graycomatrix, graycoprops\n",
    "from skimage.feature import structure_tensor, structure_tensor_eigenvalues\n",
    "from skimage.feature import hessian_matrix, hessian_matrix_eigvals\n",
    "\n",
    "def compute_surface_area(binary_mask):\n",
    "    \"\"\"Compute surface area of a 3D binary mask using marching cubes.\"\"\"\n",
    "    verts, faces, _, _ = measure.marching_cubes(binary_mask, level=0)\n",
    "    area = 0.0\n",
    "    for tri in faces:\n",
    "        p0, p1, p2 = verts[tri]\n",
    "        tri_area = 0.5 * np.linalg.norm(np.cross(p1 - p0, p2 - p0))\n",
    "        area += tri_area\n",
    "    return area, verts\n",
    "\n",
    "def compute_convex_volume(verts):\n",
    "    \"\"\"Compute convex hull volume from vertices.\"\"\"\n",
    "    try:\n",
    "        hull = ConvexHull(verts)\n",
    "        return hull.volume\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def compute_directional_glcm_features(img_3d, mask_3d, distances=[1], levels=256):\n",
    "    \"\"\"Compute simple directional GLCM contrast on three orthogonal slices through ROI.\"\"\"\n",
    "    # Take central slices through the ROI\n",
    "    coords = np.array(np.where(mask_3d))\n",
    "    zc, yc, xc = [int(np.mean(c)) for c in coords]\n",
    "\n",
    "    features = {}\n",
    "\n",
    "    # Helper to compute contrast per angle in 2D\n",
    "    def glcm_contrast(slice2d):\n",
    "        # quantize\n",
    "        img = slice2d.astype(np.uint8)\n",
    "        glcm = graycomatrix(img, distances=distances, angles=[0, np.pi/4, np.pi/2, 3*np.pi/4],\n",
    "                            levels=levels, symmetric=True, normed=True)\n",
    "        contrast = [graycoprops(glcm, 'contrast')[0, i] for i in range(4)]\n",
    "        return contrast\n",
    "\n",
    "    # XY slice\n",
    "    contrast_xy = glcm_contrast(img_3d[zc])\n",
    "    features['glcm_xy_contrast_mean'] = np.mean(contrast_xy)\n",
    "    features['glcm_xy_anisotropy'] = (max(contrast_xy) - min(contrast_xy)) / (np.mean(contrast_xy)+1e-8)\n",
    "\n",
    "    # XZ slice\n",
    "    contrast_xz = glcm_contrast(img_3d[:, yc, :])\n",
    "    features['glcm_xz_contrast_mean'] = np.mean(contrast_xz)\n",
    "    features['glcm_xz_anisotropy'] = (max(contrast_xz) - min(contrast_xz)) / (np.mean(contrast_xz)+1e-8)\n",
    "\n",
    "    # YZ slice\n",
    "    contrast_yz = glcm_contrast(img_3d[:, :, xc])\n",
    "    features['glcm_yz_contrast_mean'] = np.mean(contrast_yz)\n",
    "    features['glcm_yz_anisotropy'] = (max(contrast_yz) - min(contrast_yz)) / (np.mean(contrast_yz)+1e-8)\n",
    "\n",
    "    return features\n",
    "\n",
    "def extract_features_with_intensity_gradient(mask_3d, img_3d):\n",
    "    mask_3d = mask_3d.astype(bool)\n",
    "    props = measure.regionprops(mask_3d.astype(np.uint8), intensity_image=img_3d)\n",
    "    \n",
    "    # precompute gradients\n",
    "    gx, gy, gz = np.gradient(img_3d.astype(np.float32))\n",
    "    grad_mag = np.sqrt(gx**2 + gy**2 + gz**2)\n",
    "\n",
    "    # structure tensor\n",
    "    A_elems = structure_tensor(img_3d, sigma=1)\n",
    "    l1, l2, l3 = structure_tensor_eigenvalues(A_elems)\n",
    "    # coherence: (λ1 - λ2) / (λ1 + λ2) for the two largest\n",
    "    coherence_map = (l1 - l2) / (l1 + l2 + 1e-8)\n",
    "\n",
    "    # Hessian\n",
    "    H_elems = hessian_matrix(img_3d, sigma=1, order='rc')\n",
    "    h_eigs = hessian_matrix_eigvals(H_elems)  # returns sorted eigenvalues\n",
    "    h_eigs = np.stack(h_eigs, axis=-1)  # shape (Z,Y,X,3)\n",
    "    # simple anisotropy ratio: |λ1| / (|λ2|+|λ3|)\n",
    "    h_aniso = np.abs(h_eigs[..., 0]) / (np.abs(h_eigs[..., 1]) + np.abs(h_eigs[..., 2]) + 1e-8)\n",
    "\n",
    "    # anisotropy maps\n",
    "    coherence_vals = coherence_map[mask_3d]\n",
    "    mean_coherence = np.mean(coherence_vals)\n",
    "    std_coherence = np.std(coherence_vals)\n",
    "\n",
    "    h_aniso_vals = h_aniso[mask_3d]\n",
    "    mean_h_aniso = np.mean(h_aniso_vals)\n",
    "    std_h_aniso = np.std(h_aniso_vals)\n",
    "\n",
    "    # directional GLCM\n",
    "    glcm_feats = compute_directional_glcm_features(img_3d, mask_3d)\n",
    "\n",
    "\n",
    "    feature_list = []\n",
    "\n",
    "    for region in props:\n",
    "        volume = region.area\n",
    "\n",
    "        # geometric features\n",
    "        surf_area, verts = compute_surface_area(mask_3d)\n",
    "        sphericity = (np.pi**(1/3) * (6 * volume)**(2/3)) / surf_area if surf_area > 0 else np.nan\n",
    "        convex_volume = compute_convex_volume(verts)\n",
    "        solidity = volume / convex_volume if convex_volume and convex_volume > 0 else np.nan\n",
    "        inertia = region.inertia_tensor\n",
    "        eigvals, _ = np.linalg.eigh(inertia)\n",
    "        elongation = np.sqrt(eigvals.min() / eigvals.max()) if eigvals.max() > 0 else np.nan\n",
    "\n",
    "        # intensity features\n",
    "        intensities = img_3d[mask_3d]\n",
    "        mean_int = np.mean(intensities)\n",
    "        std_int = np.std(intensities)\n",
    "        min_int = np.min(intensities)\n",
    "        max_int = np.max(intensities)\n",
    "        med_int = np.median(intensities)\n",
    "        skew_int = skew(intensities, bias=False)\n",
    "        kurt_int = kurtosis(intensities, bias=False)\n",
    "\n",
    "        # gradient-based features\n",
    "        grad_vals = grad_mag[mask_3d]\n",
    "        mean_grad = np.mean(grad_vals)\n",
    "        std_grad = np.std(grad_vals)\n",
    "        min_grad = np.min(grad_vals)\n",
    "        max_grad = np.max(grad_vals)\n",
    "        med_grad = np.median(grad_vals)\n",
    "        skew_grad = skew(grad_vals, bias=False)\n",
    "        kurt_grad = kurtosis(grad_vals, bias=False)\n",
    "\n",
    "        vol_portion = np.sum(mask_3d) / np.prod(mask_3d.shape)\n",
    "\n",
    "        feature_list.append({\n",
    "            \"label\": region.label,\n",
    "            # morphology\n",
    "            \"volume_voxels\": volume,\n",
    "            \"vol_portion\": vol_portion,\n",
    "            \"surface_area\": surf_area,\n",
    "            \"sphericity\": sphericity,\n",
    "            \"convex_volume\": convex_volume,\n",
    "            \"solidity\": solidity,\n",
    "            \"elongation\": elongation,\n",
    "            # intensity\n",
    "            \"intensity_mean\": mean_int,\n",
    "            \"intensity_std\": std_int,\n",
    "            \"intensity_min\": min_int,\n",
    "            \"intensity_max\": max_int,\n",
    "            \"intensity_median\": med_int,\n",
    "            \"intensity_skew\": skew_int,\n",
    "            \"intensity_kurtosis\": kurt_int,\n",
    "            # gradient\n",
    "            \"grad_mean\": mean_grad,\n",
    "            \"grad_std\": std_grad,\n",
    "            \"grad_min\": min_grad,\n",
    "            \"grad_max\": max_grad,\n",
    "            \"grad_median\": med_grad,\n",
    "            \"grad_skew\": skew_grad,\n",
    "            \"grad_kurtosis\": kurt_grad,\n",
    "            # anisotropy\n",
    "            \"coherence_mean\": mean_coherence,\n",
    "            \"coherence_std\": std_coherence,\n",
    "            \"hessian_aniso_mean\": mean_h_aniso,\n",
    "            \"hessian_aniso_std\": std_h_aniso,\n",
    "        })\n",
    "        feature_list[0].update(glcm_feats)\n",
    "    return pd.DataFrame(feature_list)\n",
    "\n",
    "label_dic={'S1': 0, 'S2': 1, 'S3': 2, 'S4': 3}\n",
    "\n",
    "csv_filename = \"train_all_data_T1.csv\"\n",
    "header_written = False \n",
    "with open(csv_filename, mode='w', newline='') as csvfile:\n",
    "    writer = None\n",
    "    for img, mask, roi, sample_name in tqdm(mod_roi_list):\n",
    "        mask = mask.astype(np.uint8)\n",
    "        vendor = sample_name.split('-')[1]\n",
    "        vendor_number = vendor_to_number.get(vendor, -1)\n",
    "        df = extract_features_with_intensity_gradient(mask, img)\n",
    "        label = label_dic[sample_name.split('-')[-1]]\n",
    "        df['label'] = label\n",
    "        df['vendor'] = vendor_number\n",
    "        if not header_written:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=df.columns)\n",
    "            writer.writeheader()\n",
    "            header_written = True\n",
    "        for _, row in df.iterrows():\n",
    "            writer.writerow(row.to_dict())\n",
    "        gc.collect()\n",
    "\n",
    "print(f\"{df.shape[0]} items written to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bc6720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot feature distribution of each label\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### read in the csv file\n",
    "df_dataset = pd.read_csv('train_all_data_T1.csv')\n",
    "\n",
    "### Choose numerical features you want to plot\n",
    "features_to_plot = df_dataset.columns.drop(['label']) # Exclude 'label' and 'bbox' column\n",
    "\n",
    "### plot in all features in subplots\n",
    "\n",
    "### Loop through each feature and plot distribution\n",
    "# Plot all features in subplots for easier comparison\n",
    "num_features = len(features_to_plot)\n",
    "ncols = 4\n",
    "nrows = int(np.ceil(num_features / ncols))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(5*ncols, 4*nrows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(features_to_plot):\n",
    "    ax = axes[idx]\n",
    "    sns.kdeplot(\n",
    "        data=df_dataset,\n",
    "        x=feature,\n",
    "        hue=\"label\",\n",
    "        fill=True,\n",
    "        common_norm=False,\n",
    "        alpha=0.5,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(f\"{feature} by class\")\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel(\"Density\")\n",
    "\n",
    "# Hide unused subplots\n",
    "for j in range(idx+1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f78aec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#built a classification model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def train_classification_model(df, label_col='label'):\n",
    "    \"\"\"Train a Random Forest classifier on the dataset.\"\"\"\n",
    "    # Split features and labels\n",
    "    print('df shape:', df.shape)\n",
    "    X = df.drop(columns=[label_col])\n",
    "    y = df[label_col]\n",
    "\n",
    "    # Split into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "    # Initialize and train the model\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "    # model.fit(X_train, y_train)\n",
    "    model.fit(X, y)\n",
    "    # # calculate categorical accuracy\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Calculate categorical accuracy for each class\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    accuracy = report['accuracy']\n",
    "    print(f\"Classification Report:{report}\")\n",
    "    print(f\"Overall accuracy: {accuracy:.4f}\")\n",
    "    return model, report\n",
    "\n",
    "\n",
    "df_dataset = pd.read_csv('train_all_data.csv')\n",
    "model, report = train_classification_model(df_dataset)\n",
    "\n",
    "# Save the model with categorical accuracy in the filename\n",
    "model_path = f'RF_{report[\"accuracy\"]:.4f}_bal_GED4.pkl'\n",
    "joblib.dump(model, model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "care2025-liver-biodreamer (3.11.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
